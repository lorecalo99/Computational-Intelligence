{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "Use reinforcement learning to devise a tic-tac-toe player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution\n",
    "To accomplish the task, I used a model-free Q-learning implemented in this way:\n",
    "1) Start an agent in state $s_0$ that concides with no moves (the board is initally empty)\n",
    "2) In each state chose an action $a$ in the following way:\n",
    "    - random action with probability $\\varepsilon$ (it can be usefull to also take advantage of exploration)\n",
    "    - best action looking at the max Q-values with probability ($1-\\varepsilon$)    \n",
    "3) Make action $a$ to move in the next state and update the Q-table in the following way:\n",
    "$$\n",
    "    Q_{t+1}(s, a) = (1 - \\alpha) * Q_t(s, a) + \\alpha * (r + \\gamma * Q_t(s', a') )\n",
    "$$ \n",
    "->  Where $Q(s', a')$ is the Q-value computed given the state after the action done by the random opponent  \n",
    "(and not simply the state after the trained player's action), considering the best possible action from that state.\n",
    "\n",
    "Note that in this game I set the reward $r$ equal to: \n",
    "- 1 if the trained player wins\n",
    "- -1 if the opponent wins\n",
    "- 0 if there is a draw\n",
    "\n",
    "The RL player training is done in a first moment against a random player and then against himself,  \n",
    "whereas the evaluation is made against a random player.  \n",
    "Further explanations about functions and parameters are inserted along the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\comp_intelligence24\\comp_intel_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import combinations\n",
    "from collections import namedtuple\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o']) # this is filled with the position (range(9)) of the two players\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8] # I used the magic square to define easily the win condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tic-Tac-Toe class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(9) # 0 indicate the the absence of a move on a box of the board\n",
    "        self.current_player = 1 # current player assumes value 1 or -1\n",
    "\n",
    "    def print_board(self):\n",
    "        for i in range(9):\n",
    "            if i in [2,5,8]:\n",
    "                end_ = \"\\n\"\n",
    "            else:\n",
    "                end_ = \"|\"\n",
    "\n",
    "            if self.board[i] == 0:\n",
    "                print(\" \", end = end_)\n",
    "            if self.board[i] == 1:\n",
    "                print(\"X\", end = end_)\n",
    "            if self.board[i] == -1:\n",
    "                print(\"O\", end = end_)\n",
    "        print(\"\")\n",
    "\n",
    "    def win(self, elements):\n",
    "        # Checks if elements is winning (elements is an array )\n",
    "        magic_numbers = [MAGIC[i] for i in elements]\n",
    "        return any(sum(c) == 15 for c in combinations(magic_numbers, 3))\n",
    "\n",
    "    def state_value(self, pos: State):\n",
    "        # State evaluation used as reward\n",
    "        if self.win(pos.x): # trained player wins\n",
    "            return 1\n",
    "        elif self.win(pos.o): # opponent wins\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def available_actions(self):\n",
    "        # return the avlable actions\n",
    "        return [i for i, v in enumerate(self.board) if v == 0] # available actions are the empty positions \n",
    "\n",
    "    def make_move(self, action):\n",
    "        # insert a new symbol (+1 or -1) in the board and gives the turn to the other player\n",
    "        self.board[action] = self.current_player # put in position \"action\" a -1 or a +1 depending on the player\n",
    "        self.current_player = -self.current_player # change current player\n",
    "\n",
    "    def get_state(self):\n",
    "        # return the state of the game (positions of the symbols on the game board)\n",
    "        return State(tuple(sorted([i for i, v in enumerate(self.board) if v == 1])), # positions of the symbol +1\n",
    "                     tuple(sorted([i for i, v in enumerate(self.board) if v == -1]))) # positions of the symbol -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent that exploits Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, epsilon=0.1, alpha=0.7, gamma=0.9):\n",
    "        self.q_table = {} # q-table defined as dictionary with key (state, action) and value q-value\n",
    "        self.epsilon = epsilon # used to choose between random and best action\n",
    "        self.alpha = alpha # it controls the weight given to new information when updating q-values\n",
    "        self.gamma = gamma # discount factor\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # return a q_value given a tuple (state, action)\n",
    "        return self.q_table.get((state, action), 0)\n",
    "\n",
    "    def choose_action(self, state, available_actions):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_actions) # random action with probability epsilon\n",
    "        else:\n",
    "            # lambda function to return the action linked to the best q-value (given the state)\n",
    "            best_action = max(available_actions, key=lambda a: self.get_q_value(state, a))\n",
    "\n",
    "            return best_action # best action with probability (1-epsilon)\n",
    "\n",
    "    def update_q_value(self, states, actions, reward, available_actions):\n",
    "        #print(len(states), len(actions)) # dimensions (3,2)\n",
    "        if available_actions:\n",
    "            # compute the new q_value on the next best action after the response of the opponent\n",
    "            max_next_q_value = max(self.get_q_value(states[2], next_action) for next_action in available_actions)\n",
    "        else:\n",
    "            max_next_q_value = 0  # no more available actions\n",
    "\n",
    "        old_q_value = self.get_q_value(states[0], actions[0])\n",
    "        # model free Q-Learning update\n",
    "        self.q_table[(states[0], actions[0])] = (1-self.alpha)*old_q_value + self.alpha*(reward + self.gamma*max_next_q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent\n",
    "The training is divided in two parts:   \n",
    "- the first half is against a random player\n",
    "- the second one is against himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:17<00:00, 5867.34it/s]\n"
     ]
    }
   ],
   "source": [
    "q_agent = Agent()\n",
    "\n",
    "training_rounds = 100_000\n",
    "\n",
    "for round in tqdm(range(training_rounds)): \n",
    "    # Training against a random player\n",
    "    tic_tac_toe = TicTacToe() # initiate each time to clear the board\n",
    "    actions = [] # action of the player + opponent response action\n",
    "    states = [] # state before player action + state after player action + state after opponent response action\n",
    "    while tic_tac_toe.available_actions():\n",
    "        state = tic_tac_toe.get_state()\n",
    "        states.append(state)\n",
    "        available_actions = tic_tac_toe.available_actions()\n",
    "\n",
    "        if round <= training_rounds/2:  \n",
    "            # the trained player plays against a random player\n",
    "            if tic_tac_toe.current_player == 1:\n",
    "                action = q_agent.choose_action(state, available_actions)\n",
    "            else:\n",
    "                action = random.choice(available_actions)\n",
    "        else:\n",
    "            # the trained player plays against himself\n",
    "            action = q_agent.choose_action(state, available_actions)\n",
    "\n",
    "        tic_tac_toe.make_move(action)\n",
    "        actions.append(action)\n",
    "        reward = tic_tac_toe.state_value(tic_tac_toe.get_state()) # compute the reward\n",
    "\n",
    "        # q table update\n",
    "        if len(actions) == 2:\n",
    "            states.append(tic_tac_toe.get_state())\n",
    "            q_agent.update_q_value(states, actions, reward, tic_tac_toe.available_actions())\n",
    "            actions = []\n",
    "            states = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the agent\n",
    "In this section I evaluate the agent against a random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game stats againts a random player over 100 games:\n",
      "Wins = 100, Draws = 0, Losses = 0\n"
     ]
    }
   ],
   "source": [
    "n_games = 100\n",
    "wins = 0\n",
    "draws = 0\n",
    "losses = 0\n",
    "\n",
    "for _ in range(n_games):\n",
    "    tic_tac_toe = TicTacToe()\n",
    "    while tic_tac_toe.available_actions():\n",
    "        state = tic_tac_toe.get_state() \n",
    "        if tic_tac_toe.current_player == 1:\n",
    "            # Q-learning agent turn\n",
    "            available_actions = tic_tac_toe.available_actions()\n",
    "            action = q_agent.choose_action(state, available_actions)\n",
    "            tic_tac_toe.make_move(action) # insert a symbol (+1) in the board and change player\n",
    "        else:\n",
    "            # random player turn\n",
    "            available_actions = tic_tac_toe.available_actions()\n",
    "            action = random.choice(available_actions)\n",
    "            tic_tac_toe.make_move(action) # insert a symbol (-1) in the board and change player\n",
    "    \n",
    "    # when there are no more available moves\n",
    "    result = tic_tac_toe.state_value(tic_tac_toe.get_state())\n",
    "    if result == 1:\n",
    "        wins += 1\n",
    "    elif result == -1:\n",
    "        losses += 1\n",
    "    else:\n",
    "        draws += 1\n",
    "\n",
    "print(f\"Game stats againts a random player over {n_games} games:\")\n",
    "print(f\"Wins = {wins}, Draws = {draws}, Losses = {losses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of a game against a random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X| | \n",
      " | | \n",
      " | | \n",
      "\n",
      "X| |O\n",
      " | | \n",
      " | | \n",
      "\n",
      "X| |O\n",
      " |X| \n",
      " | | \n",
      "\n",
      "X| |O\n",
      " |X| \n",
      " | |O\n",
      "\n",
      "X| |O\n",
      "X|X| \n",
      " | |O\n",
      "\n",
      "X| |O\n",
      "X|X| \n",
      " |O|O\n",
      "\n",
      "X| |O\n",
      "X|X|X\n",
      " |O|O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tic_tac_toe = TicTacToe()\n",
    "\n",
    "while tic_tac_toe.available_actions():\n",
    "    state = tic_tac_toe.get_state() \n",
    "    if tic_tac_toe.current_player == 1:\n",
    "        # Q-learning agent turn\n",
    "        available_actions = tic_tac_toe.available_actions()\n",
    "        action = q_agent.choose_action(state, available_actions)\n",
    "    else:\n",
    "        # random player turn\n",
    "        available_actions = tic_tac_toe.available_actions()\n",
    "        action = random.choice(available_actions)\n",
    "\n",
    "    tic_tac_toe.make_move(action) # insert a symbol (+1 or -1) in the board and change player\n",
    "    tic_tac_toe.print_board()\n",
    "    \n",
    "    if tic_tac_toe.state_value(tic_tac_toe.get_state()) in [-1, 1]:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_intel_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
